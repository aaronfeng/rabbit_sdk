* settings

#+LINK: bug https://extra.lshift.net/bugzilla/show_bug.cgi?id=
#+TODO: TODO | WAITING | DONE

* todo

** client API

three layers 

We need this for .net initially, and Java at some later point

*** DONE standard RMQ client API
*** TODO reliable, available, asynchronous, point-to-point conversation API

in turn comprised of three sub-layers

**** TODO reliable connection/channel

at-least-once delivery in the presence of
- server and network failures
- sender and recipient failures 

- same api as normal connection/channel
- channels keeps track of
  - subscriptions
  - tx mode or checkpoint mode
  - qos prefetch
  - publishes since last commit/cancel / checkpoint, or during last n ms
    (depending on mode)
  - pending acks
- automatically reconnects in the event of errors that are not the
  client/app's fault
- on reconnect the channels are re-opened and for each channel
  - the tx/checkpoint mode is (re)selected
  - the qos prefetch is (re)set
  - subscriptions are recreated
  - buffered publishes are resent
- acks for messages delivered in a previous (i.e. not the current)
  session are silently dropped (or, rather, we drop all acks that
  aren't pending) - otherwise the server would reject
  them with an error
- callback to notify app when broker is deemed to have accepted
  responsibilty for a message. By persisting this info apps can limit
  the amount of resends they have to do in the event of a client
  failure.
- callbacks for connection and channel lifecycle events - invoked for
  connect, reconnect, disconnect and retry-exceeded events

props:
- reconnect/resend policy
  - retry_interval
  - exponential_backoff_steps
  - maximum retries
  - buffer_duration (keep outbound messages for n ms, in case resend
    is needed)

***** no replay of commands

The trouble with replaying commands is that we may stomp on resources
that have been created/deleted by other connections.

The other problem is that we do not know the state the system was in
at the beginning of the recording.

And, finally, some commands are not idempotent.

-> decision: we only keep track of publishes. Synchronous commands
fail in the event of a disconnect.

async commands issued during a disconnect are silently dropped, except
for publishes.
Q: should publishes be totally async, even in the presence of
  disconnects? Ideally yes, but there is a danger of buffering vast
  volumes of messages during prolonged outages.

**** TODO reliable, asynchronous, point-to-point conversation API

see [[file:unicast.org][unicast]]

- ultimately built on top of reliable connection/channel, but can also
  use an ordinary connection/channel

**** TODO HA version of the above

Q: Do we need full HA or would standby (plus maybe SAN) be sufficient?

creates queues on multiple nodes, subscribes to all of them, and acks
& throws away all received messages sent by the same sender with an id
less than or equal of an already received id. NB: the acking should
only happen after the original message has been ack'ed.

Q: how can we make this work reasonably efficient in a workload
distribution scenario?

PS: when there are multiple workers operating on a message stream,
they send replies the message ids are only unique per worker. That
means they need to identify themselves as a different *sender*, but
set the reply-to to the common address.

*** TODO algo collateral messaging API

** protocol

A -> B: msg(id)
A <- : notification(id, stage)
A <- : ...
B -> A: msg(in-reply-to:id, id)
B <- : notification(id, stage)
B <- : ...

NB: because delivery notifications are sent from multiple agents,
  message order cannot be guaranteed

*** TODO define collateral protocol
i.e. the stuff that sits on top of the above

*** TODO draw routing topology

*** TODO draw logical message flow
between C1, algo agent, C2

*** TODO sequence diagrams

*** TODO figure out messaging role of algo agent

Is it a proxy, in which case it will pass on messages with their
original from & message-id, or is it a full participant, in which case
in order to get the notifications to work the way we want it will need
to keep a mapping from message ids of the inbound messages to the
message ids of the corresponding outbound messages, so that it can
re-map the ids for notifications sent baxk by the recipient.

Let's assume it is a proxy and experiment with that.

** security

*** TODO [[http://www.enisa.europa.eu/rmra/er_use_by_example.html][Twelve Steps]]

**** Phase 1: Identification of security and privacy risks
***** I 1: Set asset protection goals
***** I 2: Define attacker model
***** I 3: Search vulnerabilities
***** I 4: Model threats
**** Phase 2: Quantification of security and privacy risks
***** Q 1: Estimate potential losses
***** Q 2: Prioritize threats
**** Phase 3: Controlling security and privacy risks
***** C 1: Select protection mechanisms
***** C 2: Implement protection mechanisms
**** Phase 4: Monitoring security and privacy risks
***** M 1: Review identified security and privacy risks
***** M 2: Review quantified security and privacy risks
***** M 3: Review impacts of implemented technical protection mechanisms
***** M 4: Report results to stakeholders

*** A start on the Twelve Steps

**** scenario

Customer C1 sends a message M to customer C2 via the Algo system
A. Other actors are customer C3 and outsider O.

**** asset protection goals

***** message confidentiality

Ideally only C1 and C2 should be able to see M - M relates to an
agreement between them, they are the only parties to the agreement,
and anybody else seeing M could gain value from that knowledge.

But A needs to be able to see M too in order to perform validation and
other value-added services. Therefore C1 and C2 need to know that A is
not doing anything other with M than what it agreed to do.

NB: by visibility we mean not just the payload but also the addressing
information, i.e. the very fact that C1 has sent M to C2, since the
latter on its own is of potential value to an attacker.

***** message authenticity

C2 needs to know that M came from C1, since an attacker could gain
value from getting C2 to do something as a result of receiving an M
that purports to be from C1 but actually isn't.

A also needs to know that M came from C1, for billing and auditing
purposes.

***** message integrity

C2 needs to know that M's content is genuine, i.e. hasn't been
tampered with, since an attacker could gain value by altering M's
content and thus C2's actions.

A also needs to know that, for the same reason, i.e. it is making
decisions based on M's content.

***** message accountability

This is very much like message authenticity, except here the objective
is for C2 (and A) to be able to prove that C1 sent M thus making it
impossible for C1 to deny that it sent M.

***** TODO server asset confidentiality and integrity, and others

We only looked at *messages* here; are there other assets that need
protecting? Probably yes, in particular various pieces of info held at
the server:
- contract info
- billing info
- various system stats - these could be of value to an algo competitor
  even if they contained no message details

*** attacker model

**** outsider
***** can tap the public IP network
***** can inject IP packets in the public network
***** can spoof IP addresses

**** customer insider
***** can do everything that outsider can
***** can gain complete control of customer-side machine

**** Algo insider
***** can do everything that outsider can
***** can gain complete control of algo systems

*** protection mechanisms

**** IP address whitelisting

This is a first line of defence, cheap to enforce at the firewall.

**** SSL for all customer <-> algo comms

This goes a long way to ensuring message confidentiality, and some way
to ensuring the other goals.

**** per-customer, strong RabbitMQ username/password

The username/password should be treated by customers in the same way
as the private key for their cert (see below), i.e. they must ensure
it doesn't get disclosed / used without appropriate authorisation.

**** restricted per-user permissions for RabbitMQ resources

- permissions set such that users cannot create/delete anything, can
  only publish to the appropriate exchanges, and can only consume from
  the appropriate queues
- queue names for clients are strong and clients are told of their
  name at provisioning time. A queue with a new name can be created
  should the the original name ever get compromised.

**** server certs

Cs trust A on the basis of the cert it supplies.

**** client-side certs

A authenticates Cs by their cert. This can happen at the firewall and
hence be (relatively, as SSL goes) cheap. Also, terminating SSL inside
rabbit would prevents external packet-level filtering.

C1 signs M with C1's cert (this stops C1 from successfully
authenticating as C1, but then sending a message claiming to be from
C3). A, at the application level, verifies the signature is genuine
and matches C1. It then re-signs the message before sending it to
C2. C2 verifies the signature of and checks that it matches
A. It *may*, also verify C1's original signature.

That way
- A asserts the authenticity, integrity and accountability of M.
- A vouches for these guarantees to C2, i.e. C2 doesn't need to
  (though it *can*) do any checks in relation to C1, just A.

We may also want A to encrypt M for C2. That would ensure that if the
rabbit credentials (username/password) of C2 get compromised, an
attacker, while they could still *steal* the messages destined for C2,
would not be able to read them.

**** preventing key compromise

It must be the responsibility of customers to ensure their private
keys are kept private.

Can consider h/w crypto, at client and at algo.

At customer, consider two- or three-factor auth.

**** no storage of unencrypted message data at algo
at least not for longer than a message is "in motion"

The idea is to limit the amount of information an attacker which has
gained *some* access to the algo system can obtain.

**** audit / certification / disclosure of algo code

With the above setup, Cs must trust A that it will do exactly what it
agreed to do, and nothing else. For example, Cs must be confident that
A won't misroute, tamper with, drop or fake messages / senders.

Some of that, namely tampering and faking, can be prevented *w/o* Cs
having to trust A - by them verifying the original sigs of the
sender. That however requires them to be in possession of the relevant
certs.

Ultimately though, some trust in A is necessary, and essentially that
means trusting the algo code - both at their end and at the server -
and the integrity of the algo systems and organisation.

It is unclear what is required for customers to gain that level of
trust, and this is likely to vary from customer to customer. But
possible options are independent audits and certification of the algo
code, or even disclosure of the algo code.

**** filtering of mis-routed messages at client

Since in many setups the code at the client end that interacts with
the algo system will be algo code (i.e. the V4/5 algo software), we
can limit the impact of accidental disclosure through mis-routing
(e.g. as a result of a bug at the server end) by getting the client to
filter out messages not destined for it.

However, this only protects against a tiny number of bugs/attacks, so
I don't think it's worth it.

**** TODO think more about client and server compromise

*** misc

**** actors vs people and systems

C*, A are not homogeneous entities - they comprise various systems,
locations, people, etc. That raises issues about who/what has access
to what data/capabilities. From an A perspective, one area where that
is of particular issue is support: how can support staff gain access
to relevant data at a) server, b) at client. For the latter, client
could log lots of stuff and pass it to A at the request of A and
consent of C.

**** cert management

***** server cert

- need to get a server cert, keep it safe and renew it as appropriate
- need to communicate the server cert to customers so they can use it
  for verification
- need to repeat this process whenever server cert changes
- for that, need a mechanism for smooth transition rather than sudden
  cut-over

***** client certs

- should probably issue certs to clients rather than asking them to
get them from some random CA. That way algo can ensure that the certs
meet all the requirements (e.g. key strength, key usage, expiry)
- need to configure firewall with the client certs
- need to configure relay app with the certs (so it can check sigs)
- need to communicate client cert to customer so they can use it for
  authentication and signing.
- need to communicate certs to *other* customers, if those customers
  want to perform verification directly (rather than trusting
  algo). NB: this may need to happen selectively - i.e. C2 should only
  get the certs of C1 and other Cs it has agreements with, not any
  other customers - depending on whether algo minds all their
  customers knowing who all the other customers are (though an
  alternative way of preventing that, since algo is in control of
  issuing the certs, is to not include any identifying information in
  the certs)
- need to repeat this process whenever client cert changes
- for that, need a mechanism for smooth transition rather than sudden
  cut-over

** protection against DOS (accidental or deliberate)

*** TODO define threat
what can a user do
- when having no credentials
- when having full credentials

*** TODO figure out how to identify misbehaving clients
- at firewall
- ordinary network monitoring
- rmq stats

*** TODO figure out how to cut off misbehaving clients
- at firewall
- by disabling their rmq account

what kind of packet-level filtering should we consider?

*** TODO RabbitMQ ulimits

*** some possible countermeasures
**** IP whitelisting as first line of defense
**** SSL client certs, checked (efficiently) provide a second line
**** what remains is rogue, authorized clients

- rate limits at firewall
- ulimits in rabbit
- limits checked by relay
- rate limits coded into client
- shutdown command that a client would react to (this can just be the
AMQP shutdown, and ultimately connection closure, but it needs to be  
triggerable in the right way)

** detecting incorrect client behaviour

*** AMQP level
- check log for errors
  - how do we tie this back to users?
    - IP
    - use rabbitmqctl connection info; but must be quick

*** app level
- Algo agent error log/reporting, for app-level errors
  - perhaps just have another X to which errors are sent

** algo agent

** provisioning tool
provisions the queue(s) for every client
provisions record of all agreements

** web i/f
*** UI interactions
*** UI design
*** back-end
*** f/e - b/e communication

** testing

** deployment

** operational monitoring

** billing

** archiving

** recovering from app-level failures
manual intervention that needs to bring the state of the three
parties back in sync

** system upgrades

** scaling

*** TODO get some estimates of baseline, peak, growth
1M msg per day + 3m notifications

<20% of agreements generate a margin call on any given day

biggest client: 20k, planned to rise to 100k
avg: 1k, expected to rise
500 clients

msg size: ?

** IM

* possible rabbit extensions

** allow suppression of queue declaration in Subscription ([[bug:21286]])

** make IBasicProperties cloneable ([[bug:21271]])

** MSBuild ([[bug:21220]])

for .net client, since nant scares Windows people.

Apparently msbuild can work under mono too.

It is useful to have an msbuild, rather than just the dll in the GAC,
because it allows source-level debugging in VS.

** DL{Q,E}

For messages that get redelivered too often. See spec of basic.deliver
for some hints. The limit & dlq name would be configured on a
per-queue basis by specifying a property at queue creation time.

The redelivery counter will need to be persistent.

NB: the advantage of DLQs over re-publishing the message to a
different exchange is that all the meta information can be preserved
in the former case whereas we'd have to create a wrapper otherwise.

OTOH, DLEs would be far more flexible...
...and we already have invented a mechanism for preserving the meta
information - namely the exchange name - for alternate exchanges.

So let's go with DLEs instead.

** stats / accounting

Record stats on usage of system

- per user connection and channel counters
- per connection frame and data volume counters (in & out)
- per channel command counter (inbound and outbound)
  - perhaps further broken down by command
- per queue msg counter (in & out)

channels and connections reference users, so aggregation by user is possible

** ulimits

- #conns per second (1st derivative of connection counter)
- #concurrent connections
- #channel creations per second (1st derivative of channel counter)
- #concurrent channels
- #commands per second (first derivative of command counter)
  - perhaps further broken down by command (ditto)
- amount of inbound data per second (first derivative of data volume counter)
NB: we don't say anything about queues here. That's because queues,
and the messages in them, aren't really owned by anybody.

For the rate-based limits, we may want to allow bursts of activity.

Since these are *u*limits, perhaps we should have a process per user
to keep track of these.

Should these limits be per cluster or per host?

** end-to-end acks

How can we get an ack all the way back to the publisher?

We could get the consumer to publish an ack message, but that seems
redundant when it is already sending and ack for the message. OTOH, an
application level ack is not always aligned with the messaging level
ack, so using the latter for the former is not always right.

** SSL ([[bug:19356]])

* possible rabbit bugs

** WSAETIMEDOUT error in CreateConnection ([[bug:21201]])
...when establishing lots of connections and running tight publish
loops in them.
[[http://www.tomshardware.com/forum/170046-46-wsaetimedout][Google says]] that this is probably due to the connection timing
out. Apparently there are some registry settings and possible params
to tweak...though it turns out that registry setting has been
removed. "using an asynchronous client socket" (google for it) may
help, though I suspect all that's going to happen is that the error
gets reported asynchronously.

** exception indicating missing inbound heartbeat in .net client ([[bug:21203]])
This happens when the client is sending a lot of messages. One reason
this may happen is if the mainloop doesn't get enough cycles.
I tried increasing the Mainloop thread priority, but that didn't make
a difference.
Running the same test on a faster machine (quad core, rather than a VM
on some old dual core), made the problem go awway :(

** when rabbit is very busy, rabbitmqctl can time out ([[bug:21202]])
with a {badrpc,timeout}


* resolved

** persistent vs non-persistent

With persistence we can shorten the duration for which a producer
needs to hang on to a message for GD - rather than having to wait
until the ultimate consumer confirms receipt, the producer just needs
to ensure it waits long enough for the message to get written to disk
by the broker.

** one user vs several

several, since it makes it easier to disable access. Also, if we only
had one username/password then if that gets compromised, potentially
allowing anybody to access the system, we'd have to ask all clients to
change their creds. Plus if we ever do add some more stats/accounting
functionality to rabbit then keying some of it on the user makes sense.

** number of queues per logical client

one - at the messaging level there is no distinction between requests,
replies and notifications.

